{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **비지도학습(Unsupervised) 알고리즘:** 군집분석\n",
    "\n",
    "---\n",
    "\n",
    "**\"`비지도학습(Unsupervised Learning)`은 `정답 레이블`이 없기 때문에, 주로 데이터를 `새롭게 표현`하여 원래 데이터보다 `쉽게 해석`하거나 `특성들을 추가적으로 파악`하는데 주로 사용\"**\n",
    "- **군집분석:** `비지도학습` 알고리즘 중 `군집화`를 위해 사용되는 가장 `기본(Baseline) 알고리즘`\n",
    "\n",
    "> (비수학적) **\"일상 속 문제들은 어떤 유형들이 있는지 파악하는 문제\"**\n",
    "> - 고객들의 정보를 통해 `성인인지 미성년자인지` `정답`을 찾는 문제가 `분류문제`\n",
    "> - 고객들의 정보를 통해 `어떤 쇼핑 취향들`이 있는지 `성인 또는 미성년자 레이블`을 할당하며 `추론`하는 문제가 `군집문제` \n",
    ">> - 데이터가 `2차원`일 경우 `시각화`를 통해 눈으로도 `패턴, 군집, 관계`를 어림짐작 가능\n",
    ">> - 데이터가 `3차원 이상`일 경우 시각화로 패턴, 군집, 관계 `파악 어려움`\n",
    "\n",
    "> (수학적) **\"특정 출력(종속변수)/입력(독립변수)의 구분이나 `관계 추론도 없고` 학습을 위한 `목표값도 없이`, 주어진 데이터를 `유사한 그룹으로 군집화(Clustering)` 하는 알고리즘\"**\n",
    "> - **분류문제:** 데이터 변수(Feature, Variable)들을 사용하여 `특정 분류값을 예측`\n",
    "> - **군집문제:** 데이터 변수(Feature, Variable)들을 사용하여 `여러개의 레이블`을 할당하면서 `특정 군집값(Cluster)을 예측`\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src='Image/Advanced/ML_Type_Application_Upgrade.png' width='700'></center>  \n",
    "<center><img src='Image/Advanced/ML_Type_Category.png' width='700'></center>\n",
    "\n",
    "---\n",
    "\n",
    "| Clustering Algorithms | Association Rule Learning Algorithms | Dimensionality Reduction Algorithms | Ensemble Algorithms | Deep Learning Algorithms |\n",
    "|------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| <img src='Image/Advanced/Clustering-Algorithms.png' width='150'> | <img src='Image/Advanced/Assoication-Rule-Learning-Algorithms.png' width='150'> | <img src='Image/Advanced/Dimensional-Reduction-Algorithms.png' width='150'> | <img src='Image/Advanced/Ensemble-Algorithms.png' width='150'> | <img src='Image/Advanced/Deep-Learning-Algorithms.png' width='150'> |\n",
    "| k-Means | Apriori algorithm | Principal Component Analysis (PCA) | Boosting | Deep Boltzmann Machine (DBM) |\n",
    "| k-Medians | Eclat algorithm | Principal Component Regression (PCR) | Bootstrapped Aggregation (Bagging) | Deep Belief Networks (DBN) |\n",
    "| Expectation Maximisation (EM) | - | Partial Least Squares Regression (PLSR) | AdaBoost | Convolutional Neural Network (CNN) |\n",
    "| Hierarchical Clustering | - | Sammon Mapping | Stacked Generalization (blending) | Stacked Auto-Encoders |\n",
    "| - | - | Multidimensional Scaling (MDS) | Gradient Boosting Machines (GBM) | - |\n",
    "| - | - | Projection Pursuit | Gradient Boosted Regression Trees (GBRT) | - |\n",
    "| - | - | Linear Discriminant Analysis (LDA) | Random Forest | - |\n",
    "| - | - | Mixture Discriminant Analysis (MDA) | - | - |\n",
    "| - | - | Quadratic Discriminant Analysis (QDA) | - | - |\n",
    "| - | - | Flexible Discriminant Analysis (FDA) | - | - |\n",
    "\n",
    "---\n",
    "\n",
    "- **종류:** 군집문제 해결 알고리즘은 다양하고, 데이터 `특성/구조/목적`에 맞는 `적절한 선택` 필요\n",
    "\n",
    "> - **Hard Clustering:** 데이터 각 `샘플(Row)`이 `여러개 군집들` 중 `하나의 군집에만 포함` 되는 것만 허용하는 방식 (`One Sample` $\\in$ `One Cluster`)\n",
    ">> - `Partitional Clustering`\n",
    "> - **Soft Clustering:** 데이터 각 `샘플(Row)`이 `여러개 군집들` 중 `중복`을 반영하여 `여러 군집에 포함` 되는 것을 허용하는 방식으로, `가중치(Weight)`나 `확률(Probability)`로서 군집에 속할 `가능성`으로 표현 (`One Sample` $\\in$ `Multiple Cluster`)\n",
    ">> - `Hierarchical Clustering`\n",
    ">> - `Density-based Clustering`\n",
    ">> - `Grid-based Clustering`\n",
    ">> - `Model-based Clustering`\n",
    ">\n",
    "| **군집특성 분류** \t| **접근방법** \t| **측정기준** \t| **알고리즘** \t| **설명** \t|\n",
    "|:---:\t|:---:\t|:---:\t|:---\t|:---\t|\n",
    "| **Hard Clustering** \t| **Partitional Clustering** \t| **-** \t| - \t| 전체 데이터를 특정 기준에 의해 한번에 구분하는 방식 \t|\n",
    "|  \t|  \t| **Distance-based** \t| K-Means \t|  \t|\n",
    "|  \t|  \t|  \t| K-Median \t|  \t|\n",
    "|  \t|  \t|  \t| K-Medoid \t|  \t|\n",
    "|  \t|  \t|  \t| Fuzzy Clustering \t|  \t|\n",
    "|  \t|  \t|  \t| PAM(Partitioning Around Medoids) \t|  \t|\n",
    "|  \t|  \t|  \t| CLARA(Clustering LARge Applications) \t|  \t|\n",
    "|  \t|  \t|  \t| CLARANS(Clustering Large Applications based on RANdomized Search) \t|  \t|\n",
    "| **Soft Clustering** \t| **Hierarchical Clustering** \t| **-** \t| - \t| 가까운 데이터들을 차근차근 묶어나가는 방식 \t|\n",
    "|  \t|  \t| **Agglomerative<br>     (Bottom-up)** \t| - \t| 개별 데이터에서 유사한 데이터끼리 묶어가는 방식 \t|\n",
    "|  \t|  \t|  \t| Single Linkage(Graph-based) \t| 각 군집에서 1개씩 뽑앗을 때 나타날 수 있는 최솟값으로 군집간 거리측정 \t|\n",
    "|  \t|  \t|  \t| Complete Linkage(Graph-based) \t| 각 군집에서 1개씩 뽑앗을 때 나타날 수 있는 최댓값으로 군집간 거리측정 \t|\n",
    "|  \t|  \t|  \t| Average Linkage(Graph-based) \t| 모든 데이터에 대한 군집간 거리평균 측정 \t|\n",
    "|  \t|  \t|  \t| Centroid Linkage(Distance-based) \t| 2개의 군집간 중심사이의 거리 측정 \t|\n",
    "|  \t|  \t|  \t| Ward Linkage(Distance-based) \t| 군집 내 오차 제곱합 기반으로 측정(유일하게 군집내 거리 기반) \t|\n",
    "|  \t|  \t|  \t| AGNES(AGglomerative NESting) \t|  \t|\n",
    "|  \t|  \t| **Divisive<br>     (Top-down)** \t| - \t| 모든 데이터를 하나의 군집이라 가정 후 세부 군집으로 분리하는 방식 \t|\n",
    "|  \t|  \t|  \t| DIANA(DIvisive ANAlysis) \t| AGNES의 역순으로 측정 \t|\n",
    "|  \t|  \t|  \t| BIRCH(Balanced Iterative Reducint and Clustering Using Hierarchies) \t| Clustering Feature Tree 기반으로 양질의 데이터에 좋은 성능 \t|\n",
    "|  \t|  \t|  \t| CURE(Clustering Using Representatives) \t| 이상치에 대해서 굉장히 Robust하게 설계 \t|\n",
    "|  \t|  \t|  \t| Chameleon \t| CURE & DBSCAN 보다 좋은 성능이며 임의 형태도 잘 찾으나 컴퓨팅 느림 \t|\n",
    "|  \t| **Density-based Clustering** \t| **-** \t| - \t| 거리 기반 군집화는 임의 형태 군집 추정에 어려우며 형태 반영을 위한 방식 \t|\n",
    "|  \t|  \t|  \t| DBSCAN(Density Based Spatial Clustering of Applications with   Noise) \t| Kmeans는 거리기준 구분, DBSCAN은 데이터밀도 높은부분 군집화 방식 \t|\n",
    "|  \t|  \t|  \t| OPTICS(Ordering Points To Identify the Clustering Structure) \t|  \t|\n",
    "|  \t|  \t|  \t| DENCLUE(DENsity-based CLUstEring) \t|  \t|\n",
    "|  \t|  \t|  \t| Density-peaks \t|  \t|\n",
    "|  \t|  \t|  \t| Robust-DB(Density Based) \t|  \t|\n",
    "|  \t| **Grid-based Clustering** \t| **-** \t| - \t| 유한개의 공간격자로 매핑하여 빠른 처리 가능한 방식 \t|\n",
    "|  \t|  \t|  \t| STING(Statistical Information Grid) \t|  \t|\n",
    "|  \t|  \t|  \t| WaveCluster \t|  \t|\n",
    "|  \t|  \t|  \t| CLIQUE(CLustering In QUEst) \t|  \t|\n",
    "|  \t| **Model-based Clustering** \t| **-** \t| - \t| 군집에 모델을 가정하기에 통계적 또는 인공신경망적 접근 방식 \t|\n",
    "|  \t|  \t| **Distribution-based** \t| Gaussian Mixture Algorithm \t|  \t|\n",
    "|  \t|  \t|  \t| Expectation Maximization Algorithm \t|  \t|\n",
    "|  \t|  \t|  \t| AutoClass(Mixture of Naïve Bayes) \t|  \t|\n",
    "|  \t|  \t|  \t| Cobweb \t|  \t|\n",
    "|  \t|  \t| **Network-based** \t| Kohonen Clustering \t|  \t|\n",
    "|  \t|  \t|  \t| SOM(Self-Organizing Map) \t|  \t|\n",
    "\n",
    "---\n",
    "\n",
    "- **Target Algorithm:**\n",
    "\n",
    "> **(1) `Partitional Clustering` vs `Hierarchical Clustering`**\n",
    ">\n",
    "> - **Partitional:** `전체 데이터`를 `Hard Clustering` 기준으로 한번에 `군집형성`하는 방식\n",
    "> - **Hierarchical:** `각각의 데이터`에서 유사성 척도(Similarity Measure)에 의해 `가까운 데이터들을` `Tree 형태`의 `계층적 군집`으로 차근차근 `묶어나가는` 방식이며 Tree에서 `어느 수준을 기준`으로 하느냐에 따라 `군집이 달라짐`\n",
    ">\n",
    "> <center><img src='Image/Advanced/Partitional_Hierarchical.png' width='600'>(https://quantdare.com/hierarchical-clustering/)</center>\n",
    ">\n",
    "> **(2) `Hierarchical Clustering`**\n",
    ">\n",
    "> - **Agglomerative(Bottom-up):** `개별 데이터`에서 `유사한 데이터끼리 묶어가는` 방식\n",
    "> - **Divisive(Top-down):** `모든 데이터`를 하나의 군집이라 가정 후 `세부 군집으로 분리`하는 방식\n",
    ">\n",
    "> <center><img src='Image/Advanced/AggloDivHierarClustering.png' width='600'>(https://quantdare.com/hierarchical-clustering/)</center>\n",
    ">\n",
    "| **군집특성 분류** \t| **접근방법** \t| **측정기준** \t| **알고리즘** \t|\n",
    "|:---:\t|:---:\t|:---:\t|:---:\t|\n",
    "| **Hard Clustering** \t| **Partitional Clustering** \t| **Distance-based** \t| `K-Means` \t|\n",
    "|  \t|  \t|  \t| `K-Median` \t|\n",
    "|  \t|  \t|  \t| `K-Mode` \t|\n",
    "|  \t|  \t|  \t| `K-Medoid` \t|\n",
    "|  \t|  \t|  \t| Fuzzy Clustering \t|\n",
    "|  \t|  \t|  \t| PAM(Partitioning Around Medoids) \t|\n",
    "|  \t|  \t|  \t| CLARA(Clustering LARge Applications) \t|\n",
    "|  \t|  \t|  \t| CLARANS(Clustering Large Applications based on RANdomized Search) \t|\n",
    "| **Soft Clustering** \t| **Hierarchical Clustering** \t| **Agglomerative<br>     (Bottom-up)** \t| Single Linkage(Graph-based) \t|\n",
    "|  \t|  \t|  \t| Complete Linkage(Graph-based) \t|\n",
    "|  \t|  \t|  \t| Average Linkage(Graph-based) \t|\n",
    "|  \t|  \t|  \t| Centroid Linkage(Distance-based) \t|\n",
    "|  \t|  \t|  \t| Ward Linkage(Distance-based) \t|\n",
    "|  \t|  \t|  \t| AGNES(AGglomerative NESting) \t|\n",
    "|  \t|  \t| **Divisive<br>     (Top-down)** \t| DIANA(DIvisive ANAlysis) \t|\n",
    "|  \t|  \t|  \t| BIRCH(Balanced Iterative Reducint and Clustering Using Hierarchies) \t|\n",
    "|  \t|  \t|  \t| CURE(Clustering Using Representatives) \t|\n",
    "|  \t|  \t|  \t| Chameleon \t|\n",
    "|  \t| **Density-based Clustering** \t|  \t| DBSCAN(Density Based   Spatial Clustering of Applications with Noise) \t|\n",
    "|  \t|  \t|  \t| OPTICS(Ordering Points To   Identify the Clustering Structure) \t|\n",
    "|  \t|  \t|  \t| DENCLUE(DENsity-based   CLUstEring) \t|\n",
    "|  \t|  \t|  \t| Density-peaks \t|\n",
    "|  \t|  \t|  \t| Robust-DB(Density Based) \t|\n",
    "|  \t| **Grid-based Clustering** \t|  \t| STING(Statistical   Information Grid) \t|\n",
    "|  \t|  \t|  \t| WaveCluster \t|\n",
    "|  \t|  \t|  \t| CLIQUE(CLustering In QUEst) \t|\n",
    "|  \t| **Model-based Clustering** \t| **Distribution-based** \t| Gaussian Mixture Algorithm \t|\n",
    "|  \t|  \t|  \t| Expectation Maximization   Algorithm \t|\n",
    "|  \t|  \t|  \t| AutoClass(Mixture of Naïve   Bayes) \t|\n",
    "|  \t|  \t|  \t| Cobweb \t|\n",
    "|  \t|  \t| **Network-based** \t| Kohonen Clustering \t|\n",
    "|  \t|  \t|  \t| SOM(Self-Organizing Map) \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **함수세팅 및 추정 방향(Modeling):** K-centroid Clustering\n",
    "\n",
    "> **\"`K`는 군집의 수를 의미하는 파라미터(Parameter)로 `분석가가 미리 정해야 함`\"**\n",
    "> - **`K-means Clustering`**\n",
    "> - **`K-median Clustering`**\n",
    "> - **`K-mode Clustering`**\n",
    "> - **`K-medoid Clustering`**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 군집문제 해결을 위한 세팅 및 추정\n",
    "\n",
    "**1) 알고리즘 함수세팅:** 군집문제를 푸는 대표적인 알고리즘 `K-means Clustering`\n",
    "\n",
    "> - 데이터의 영역을 대표하는 `클러스터 중심 찾기`\n",
    "> - 가장 `단순하고 빠른 군집화` 방법의 하나\n",
    "> - 비교적 `이해하기 쉽고` 구현도 쉬워 `가장 인기` 있는 군집 알고리즘\n",
    ">\n",
    "> (1) **Initialization:** `임의 공간 및 갯수`의 `클러스터 중심을 할당`\n",
    ">\n",
    "> (2) **Assignment:** `클러스터 중심`으로부터 `모든 데이터`에 대해 `가까운 클러스터`로 군집화 한다\n",
    ">\n",
    "> (3) **Update:** 각 클러스터 `내부 데이터들의 평균`으로 `중심 재지정`\n",
    ">\n",
    "> (4) **Finalization:** `클러스터 중심`이 변화가 없거나 `비용함수`가 일정 수치 이하가 될때까지 `Assignment & Update`를 반복\n",
    ">\n",
    "> <center><img src='Image/Advanced/Clustering_Kmeans.png' width='600'></center>\n",
    ">\n",
    "> <center><img src='Image/Advanced/Clustering_Kmeans_Video.gif' width='400'>(Wikipedia)</center>\n",
    "\n",
    "---\n",
    "\n",
    "**2) 함수 추정을 위한 비용함수:** \n",
    "\n",
    "> **\"특정 출력(종속변수)/입력(독립변수)의 구분이나 `관계 추론도 없고` 학습을 위한 `목표값도 없이`, 주어진 데이터의 `레이블을 할당`하면서 `유사한 그룹으로 군집화(Clustering)` 하는 알고리즘\"**\n",
    "\n",
    "- **이슈: 군집의 `정답레이블이 없어` 지도학습과 달리 `명확한 모델링 평가 쉽지 않음`**    \n",
    "- **방향:** 군집 `내부 유사성은 높임` + `외부` 군집들과의 `유사성은 낮춤`\n",
    "\n",
    "> **\"`응집도`와 `분리도`를 모두 만족하는 군집을 찾는 `최적화 과정`\"**\n",
    "> - **응집도(Cohesion):** 군집 중심과 `내부 데이터들의 유사성(거리)` 합이 `최소화`\n",
    ">> - 군집 내 분산(Inner-cluster Variance) 최소화\n",
    "> - **분리도(Seperation):** `군집들` 간의 `비유사성` 합이 `최대화`\n",
    ">> - 군집 간 분산(Inter-cluster Variance) 최대화\n",
    ">\n",
    "> <center><img src='Image/Advanced/Cohesion_Separation.png' width='500'></center>\n",
    "\n",
    "- **유사성 측정:** \n",
    "\n",
    "> - **컴퓨터의 데이터 분류:**\n",
    "> \n",
    "| **대분류** \t| **소분류** \t| **컴퓨터의 분류1** \t| **컴퓨터의 분류2** \t|\n",
    "|:---:\t|:---:\t|:---:\t|:---:\t|\n",
    "| **질적변수(Qualitative Variable)** \t| **-** \t| - | 범주형 |\n",
    "|  \t| **명목형 변수(Nominal Variable)** \t|  문자\t|  범주형 |\n",
    "|  \t| **순위형 변수(Ordinal Variable)** \t|  숫자\t|  범주형 |\n",
    "| **양적변수(Quantitative Variable)** \t| **-** \t| - | 연속형 |\n",
    "|  \t| **이산형 변수(Discrete Variable)** \t|  숫자\t| 연속형 |\n",
    "|  \t| **연속형 변수(Continuous Variable)** \t|  숫자\t| 연속형 |\n",
    ">\n",
    "> - **측정 지표:** \n",
    ">\n",
    "| **변수 종류** \t| **측정** \t| **설명** \t|\n",
    "|:---:\t|:---:\t|:---:\t|\n",
    "| **Continuous Variable** \t| **Manhattan Distance(Minkowski at Rank=1)** \t| 최단 루트 측정(변수들의 단위가 다르거나 상관성이 있으면 크게 변함) \t|\n",
    "|  \t| **Euclidean Distance(Minkowski at Rank=2)** \t| 최단 거리 측정(변수들의 단위가 다르거나 상관성이 있으면 크게 변함) \t|\n",
    "|  \t| **Standardized Distance** \t| 변수의 분산을 고려하여 표준화 측정 \t|\n",
    "|  \t| **Mahalanobis Distance** \t| 변수의 표준화 및 변수들의 상관관계 측정 \t|\n",
    "|  \t| **Weighted Euclidean Distance** \t| Euclidean & Standardized의 일반화 측정 \t|\n",
    "| **Continuous/Discrete Variable** \t| **Pearson's Correlation Coefficient** \t| 상관관계 측정 \t|\n",
    "| **Discrete(Binary)/Nominal Variable** \t| **Simple Matching Coefficient** \t| 수식 참고 \t|\n",
    "|  \t| **Jaccard's Coefficient** \t| 수식 참고 \t|\n",
    "|  \t| **Russell and Rao Coefficient** \t| 수식 참고 \t|\n",
    "| **Nominal Variable** \t| **Cosine Distance** \t| 문자 벡터들의 각도 측정 \t|\n",
    "|  \t| **Levenshtein Metric** \t| 문자 벡터들에서 다른 단어로 변경시 필요한 편집수 측정 \t|\n",
    "|  \t| **Tanimoto Coefficient(Expanded Jaccard's Coefficient)** \t| 문자 벡터 적용 Jaccard's Coefficient \t|\n",
    "| **Ordinal Variable** \t| **Rank Correlation Coefficient** \t| 순위기반 상관관계 측정 \t|\n",
    "| **Continuous/Discrete/Nominal/Ordinal** \t| **Hamming Distance** \t| 같은 길이의 데이터에 같은 위치에 있는 값들의 비교 측정 \t|\n",
    ">\n",
    "> - **비용함수:** `K-means Clustering`은 `Euclidean Distance`를 비용함수로 `최적화`\n",
    ">\n",
    "> $$\n",
    "\\begin{aligned}\n",
    "Cost &= \\sum_{i=1}^{N}\\min_{\\mu_j \\in C}(||x_i - \\mu_j||^2) \\\\\n",
    "& \\text{where } C \\text{ is the estimated cluster,} \\\\\n",
    "& \\text{and } \\mu_j \\text{ is the average center of } j \\text{-th cluster.}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**3) K-centroid Clustering:**\n",
    "\n",
    "> - **`K-means Clustering`:** 비용함수에서 `평균` 사용 군집화\n",
    "> - **`K-median Clustering`:** 비용함수에서 `중앙값(Median)` 사용 군집화\n",
    "> - **연속형 데이터**: `K-means Clustering` 또는 `K-median Clustering`\n",
    "> - **이산형 데이터**: `K-mode Clustering` 또는 `K-medoid Clustering`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **군집문제 해결:** K-means & Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "plt.plot([1,2,4,5], [1,1,3,4], 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) 반복1:**\n",
    "> (1) **Initialization:** 데이터 중 `임의 2개 중심 초기값` 선택 - `(1,1), (2,1)`\n",
    ">\n",
    "> (2-1) **Assignment:** `Euclidean` 거리 계산 - $\\sqrt{(x_{중심} - x)^2 + (y_{중심} - y)^2}$ \n",
    ">\n",
    "| **중심 초기값** \t| **샘플1** \t| **샘플2** \t| **샘플3** \t| **샘플4** \t|\n",
    "|:---:\t|:---:\t|:---:\t|:---:\t|:---:\t|\n",
    "| (1,1) \t| 0 \t| 1 \t| 3.6 \t| 5 \t|\n",
    "| (2,1) \t| 1 \t| 0 \t| 2.8 \t| 4.2 \t|\n",
    ">\n",
    "> (2-2) **Assignment:** `클러스터` 확인 - $\\{ \\mathbf{샘플}_1 \\}, \\{ \\mathbf{샘플}_2, \\mathbf{샘플}_3, \\mathbf{샘플}_4 \\}$\n",
    ">\n",
    "| **중심 초기값** \t| **샘플1** \t| **샘플2** \t| **샘플3** \t| **샘플4** \t|\n",
    "|:---:\t|:---:\t|:---:\t|:---:\t|:---:\t|\n",
    "| (1,1) \t| 0.0 \t| 1.0 \t| 3.6 \t| 5.0 \t|\n",
    "| (2,1) \t| 1.0 \t| 0.0 \t| 2.8 \t| 4.2 \t|\n",
    "| 클러스터 중심 \t| (1,1) \t| (2,1) \t| (2,1) \t| (2,1) \t|\n",
    ">\n",
    "> (3) **Update:** 중심 `초기값 재지정` - `(1,1), (3.7,2.7)`\n",
    ">\n",
    "> $$\n",
    "\\begin{aligned}\n",
    "\\hat{\\boldsymbol{\\mu}}_1(1) &= \\mathbf{샘플}_1 = (1,1) \\\\\n",
    "\\hat{\\boldsymbol{\\mu}}_2(1) &= \\frac{\\mathbf{샘플}_2 + \\mathbf{샘플}_3 + \\mathbf{샘플}_4}{3} = (3.7, ~2.7)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**2) 반복2:**\n",
    "> (2-1) **Assignment:** `Euclidean` 거리 계산 - $\\sqrt{(x_{중심} - x)^2 + (y_{중심} - y)^2}$ \n",
    ">\n",
    "| **중심 초기값** \t| **샘플1** \t| **샘플2** \t| **샘플3** \t| **샘플4** \t|\n",
    "|:---:\t|:---:\t|:---:\t|:---:\t|:---:\t|\n",
    "| (1,1) \t| 0.0 \t| 1.0 \t| 3.6 \t| 5.0 \t|\n",
    "| (3.7,2.7) \t| 3.1 \t| 2.4 \t| 0.5 \t| 1.9 \t|\n",
    ">\n",
    "> (2-2) **Assignment:** `클러스터` 확인 - $\\{ \\mathbf{샘플}_1, \\mathbf{샘플}_2 \\}, \\{ \\mathbf{샘플}_3, \\mathbf{샘플}_4 \\}$\n",
    ">\n",
    "| **중심 초기값** \t| **샘플1** \t| **샘플2** \t| **샘플3** \t| **샘플4** \t|\n",
    "|:---:\t|:---:\t|:---:\t|:---:\t|:---:\t|\n",
    "| (1,1) \t| 0.0 \t| 1.0 \t| 3.6 \t| 5.0 \t|\n",
    "| (3.7,2.7) \t| 3.1 \t| 2.4 \t| 0.5 \t| 1.9 \t|\n",
    "| 클러스터 중심 \t| (1,1) \t| (1,1) \t| (3.7,2.7) \t| (3.7,2.7) \t|\n",
    ">\n",
    "> (3) **Update:** 중심 `초기값 재지정` - `(1.5,1), (4.5,3.5)`\n",
    ">\n",
    "> $$\n",
    "\\begin{aligned}\n",
    "\\hat{\\boldsymbol{\\mu}}_1(2) &= \\frac{\\mathbf{x}_1 + \\mathbf{x}_2}{2} = (1.5,~1) \\\\\n",
    "\\hat{\\boldsymbol{\\mu}}_2(2) &= \\frac{\\mathbf{x}_3 + \\mathbf{x}_4}{2} = (4.5, ~3.5)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**3) 반복3:**\n",
    "> (2-1) **Assignment:** `Euclidean` 거리 계산 - $\\sqrt{(x_{중심} - x)^2 + (y_{중심} - y)^2}$ \n",
    ">\n",
    "| **중심 초기값** \t| **샘플1** \t| **샘플2** \t| **샘플3** \t| **샘플4** \t|\n",
    "|:---:\t|:---:\t|:---:\t|:---:\t|:---:\t|\n",
    "| (1.5,1)\t| 0.5 \t| 0.5 \t| 3.2 \t| 4.6 \t|\n",
    "| (4.5,3.5) \t| 4.3 \t| 3.5 \t| 0.7 \t| 0.7 \t|\n",
    ">\n",
    "> (2-2) **Assignment:** `클러스터` 확인 - $\\{ \\mathbf{샘플}_1, \\mathbf{샘플}_2 \\}, \\{ \\mathbf{샘플}_3, \\mathbf{샘플}_4 \\}$\n",
    ">\n",
    "| **중심 초기값** \t| **샘플1** \t| **샘플2** \t| **샘플3** \t| **샘플4** \t|\n",
    "|:---:\t|:---:\t|:---:\t|:---:\t|:---:\t|\n",
    "| (1.5,1)\t| 0.5 \t| 0.5 \t| 3.2 \t| 4.6 \t|\n",
    "| (4.5,3.5) \t| 4.3 \t| 3.5 \t| 0.7 \t| 0.7 \t|\n",
    "| 클러스터 중심 \t| (1.5,1) \t| (1.5,1)\t| (4.5,3.5) \t| (4.5,3.5) \t|\n",
    ">\n",
    "> (4) **Finalization:** 클러스터 중심의 변화가 없으므로 `알고리즘 종료` - $\\{ \\mathbf{샘플}_1, \\mathbf{샘플}_2 \\}, \\{ \\mathbf{샘플}_3, \\mathbf{샘플}_4 \\}$\n",
    ">\n",
    "> <center><img src='Image/Advanced/Clustering_Kmeans_Example.png' width='600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 군집문제 한계\n",
    "\n",
    "> **1) `비지도학습` 문제임에도 불구하고 `K-means`는 `클러스터의 갯수` `k`를 `미리 지정`해야 함**\n",
    "> - `k` 갯수에 따라서 `결과는 천차만별`이지만 `k` 갯수를 추정하지 못하는 것이 `결정적 한계`\n",
    "> - `k` 갯수 추론을 위한 `별도의 방법론`을 고려해야 함 (`Elbow Method`, `Silhouette Method`)\n",
    "> ```python\n",
    ">from sklearn.cluster import KMeans\n",
    ">from yellowbrick.cluster import KElbowVisualizer\n",
    ">model = KMeans()\n",
    ">visualizer = KElbowVisualizer(model, k=(1,10))\n",
    ">visualizer.fit(df)\n",
    ">```\n",
    ">\n",
    "> **2) 처음에 정하는 `초기값`에 따라서 `결과도 달라질 수` 있고 `알고리즘 성능에 영향`을 줌**\n",
    "> - `고객 세분화(Segmentation)`에서 매번 `초기값에 따라` 묶이는 `고객 그룹이 달라질 수` 있음\n",
    "> - `잘못된 초기값`은 `무한/많은 반복과정`을 유발시키거나 `군집 성능을 떨어뜨릴 수` 있음\n",
    "> - `적절한 초기값`을 결정하기 위한 `방법론`을 고려해야 함(`K-means++`)\n",
    ">> (1) `K-means++`는 `임의 공간` 중심 초기값 대신 `데이터들 중 무작위로 1개`의 중심 초기값 선택\n",
    ">>\n",
    ">> (2) `기 지정된 초기값`에서 `최대한 먼 곳`에 배치된 데이터를 다음 중심 초기값으로 지정\n",
    ">>\n",
    ">> (3) `k`개의 `중심 초기값`이 선택될 때까지 위 과정을 `반복`\n",
    "> - `K-means++` 방식 적용시, 기존보다 알고리즘의 `반복이 줄어들어 속도가 빨라지고 성능이 좋아짐`\n",
    "> ```python\n",
    ">from sklearn.cluster import KMeans\n",
    ">model = KMeans(n_clusters=k,  init='k-means++')\n",
    ">```\n",
    ">\n",
    "> **3) 구분된 군집이 `일부 데이터에선 최적`일 수 있으나 `전체 데이터에선 최적(Global Optima)을 보장하지 않는` `지역 최적화 알고리즘(Local Optimization Algorithm)`**\n",
    ">\n",
    "> **4) `평균계산` 방식이라 일부 `Outlier`로 인해 결과가 `달라질 수` 있음**\n",
    "> - `평균`보다 `중앙값`이 이상치에 덜 민감하므로 `K-median Clustering`이 `K-means Clustering`의 대안이 될 수 있음\n",
    "> - `데이터 탐색 과정`에서 `이상치 제거`하는 것도 방법\n",
    ">\n",
    "> **5) `Euclidean Distance` 방식이라 `원형/구형 클러스터 기준`으로 군집을 구분하기 때문에, 다른 `도넛형, 사각형, 럭비공형 등`의 다양한 모양이나 `다양한 밀도`의 클러스터 인식에는 `효과가 없음`**\n",
    ">\n",
    "> <center><img src='Image/Advanced/Kmeans_Limit.png' width='900'></center>\n",
    ">\n",
    "> **6) `Hard Clustering` 방식이라 `여러 클러스터에 포함될 수 있는 데이터`를 고려하지 못함**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch K-means Clustering\n",
    "\n",
    "> - `K-means Clustering`은 `중심위치와 모든 데이터 사이의 거리`를 계산해야 하기 때문에 `데이터의 갯수`가 많아지면 `계산량도 늘어남`\n",
    "> - 데이터의 수가 너무 많을 때는 `미니배치 K-평균(Mini-batch) 군집화 방법`을 사용하면 계산량을 `줄일 수 있음`\n",
    "> - 데이터를 `미니배치 크기만큼 무작위로 분리`하여 `K-means Clustering`\n",
    "> - `모든 데이터`를 사용했을 경우와 `결과가 다를 수 있지만 큰 차이 없음`\n",
    "> - `scikit-learn`의 `cluster` 패키지는 `MiniBatchKMeans` 클래스를 제공하여 `큰 대용량 데이터셋`에서도 잘 작동\n",
    ">\n",
    "> ```python\n",
    ">from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    ">model = KMeans(n_clusters=k,  init='k-means++')\n",
    "># 대신\n",
    ">model = MiniBatchKMeans(n_clusters=k, batch_size=1000)\n",
    ">```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c719e123d9f928ac52be215099cb039213d76cae257ca45f86a463b29cea7241"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
